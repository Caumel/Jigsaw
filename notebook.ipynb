{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caumel/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./jigsaw-toxic-comment-train-processed-seqlen128.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>input_word_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>all_segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 27746, 31609, 11809, 24781, 10105, 70971...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 141, 112, 56237, 10874, 106, 10357, 1825...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 35936, 10817, 117, 146, 112, 181, 30181,...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 107, 15946, 146, 10944, 112, 188, 13086,...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 11065, 117, 52523, 117, 10301, 15127, 51...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223544</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 131, 43325, 117, 146, 12888, 13028, 1479...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223545</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 134, 134, 22716, 19111, 101695, 134, 134...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223546</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 134, 134, 48201, 18969, 10135, 13028, 10...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223547</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 47430, 11369, 144, 72918, 10731, 30118, ...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223548</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(101, 107, 134, 134, 22966, 69212, 84971, 3041...</td>\n",
       "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223549 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "223544  fff8f64043129fa2  :Jerome, I see you never got around to this…! ...   \n",
       "223545  fff9d70fe0722906  ==Lucky bastard== \\n http://wikimediafoundatio...   \n",
       "223546  fffa8a11c4378854  ==shame on you all!!!== \\n\\n You want to speak...   \n",
       "223547  fffac2a094c8e0e2  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...   \n",
       "223548  fffb5451268fb5ba  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "223544      0             0        0       0       0              0   \n",
       "223545      0             0        0       0       0              0   \n",
       "223546      0             0        0       0       0              0   \n",
       "223547      1             0        1       0       1              0   \n",
       "223548      0             0        0       0       0              0   \n",
       "\n",
       "                                           input_word_ids  \\\n",
       "0       (101, 27746, 31609, 11809, 24781, 10105, 70971...   \n",
       "1       (101, 141, 112, 56237, 10874, 106, 10357, 1825...   \n",
       "2       (101, 35936, 10817, 117, 146, 112, 181, 30181,...   \n",
       "3       (101, 107, 15946, 146, 10944, 112, 188, 13086,...   \n",
       "4       (101, 11065, 117, 52523, 117, 10301, 15127, 51...   \n",
       "...                                                   ...   \n",
       "223544  (101, 131, 43325, 117, 146, 12888, 13028, 1479...   \n",
       "223545  (101, 134, 134, 22716, 19111, 101695, 134, 134...   \n",
       "223546  (101, 134, 134, 48201, 18969, 10135, 13028, 10...   \n",
       "223547  (101, 47430, 11369, 144, 72918, 10731, 30118, ...   \n",
       "223548  (101, 107, 134, 134, 22966, 69212, 84971, 3041...   \n",
       "\n",
       "                                               input_mask  \\\n",
       "0       (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "223544  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "223545  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "223546  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "223547  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "223548  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           all_segment_id  \n",
       "0       (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "223544  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "223545  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "223546  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "223547  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "223548  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[223549 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'input_word_ids', 'input_mask',\n",
       "       'all_segment_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'id', \n",
    "- 'comment_text', # Text\n",
    "- 'toxic', # Si es toxico o no\n",
    "- 'severe_toxic', ?\n",
    "- 'obscene', ?\n",
    "- 'threat', ?\n",
    "- 'insult', ?\n",
    "- 'identity_hate', ?\n",
    "- 'input_word_ids',  # Tokens\n",
    "- 'input_mask', # Si es importante o no\n",
    "- 'all_segment_id' # Si cuenta con mas de una frase ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,:][\"input_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(101, 27746, 31609, 11809, 24781, 10105, 70971, 10107, 11019, 10571, 15127, 29115, 23920, 72832, 56684, 30126, 10309, 86095, 46949, 136, 11696, 10309, 10115, 112, 188, 109995, 33269, 12387, 117, 12820, 69177, 10135, 11152, 74212, 10107, 10662, 146, 34584, 10160, 10287, 10482, 80332, 20794, 10858, 119, 12689, 20648, 42658, 10112, 16938, 112, 188, 51600, 10105, 101986, 23953, 10188, 10105, 31311, 15975, 11764, 146, 112, 181, 18675, 11858, 119, 12642, 119, 20862, 119, 11171, 119, 10365, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.at[0, 'input_word_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Entreno ?\n",
    "# Fine-tune\n",
    "\n",
    "# Uso el default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    return re.sub(r\"[.,!?-]\", '', text.lower()).replace(\"\\n\",\" \")\n",
    "\n",
    "def string_to_tensor(s):\n",
    "    return torch.tensor(literal_eval(s), dtype=torch.long)\n",
    "\n",
    "df['input_word_ids'] = df['input_word_ids'].apply(string_to_tensor)\n",
    "df['input_mask'] = df['input_mask'].apply(string_to_tensor)\n",
    "df['all_segment_id'] = df['all_segment_id'].apply(string_to_tensor)\n",
    "# df['comment_text'] = df['comment_text'].apply(preprocessing)\n",
    "# df['comment_text'] = df['comment_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512 \n",
    "\n",
    "#Controlo el maximo tamaño del input ya que BERT solo permite 512\n",
    "\n",
    "def truncate_or_pad(tensor, max_length):\n",
    "    # Truncar si el tensor es más largo que max_length\n",
    "    if len(tensor) > max_length:\n",
    "        return tensor[:max_length]\n",
    "    # Rellenar con ceros si el tensor es más corto que max_length\n",
    "    elif len(tensor) < max_length:\n",
    "        pad_length = max_length - len(tensor)\n",
    "        return torch.cat((tensor, torch.zeros(pad_length, dtype=torch.long)))\n",
    "\n",
    "    return tensor\n",
    "\n",
    "df['input_word_ids'] = df['input_word_ids'].apply(lambda x: truncate_or_pad(x, max_length))\n",
    "df['input_mask'] = df['input_mask'].apply(lambda x: truncate_or_pad(x, max_length))\n",
    "df['all_segment_id'] = df['all_segment_id'].apply(lambda x: truncate_or_pad(x, max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 30522\n"
     ]
    }
   ],
   "source": [
    "# Controlo el vocabulario de BERT, para saber si estan entre los indices del modelo, para que no exista palabra que el modelo no entienda.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(\"Tamaño del vocabulario:\", vocab_size)\n",
    "\n",
    "def adjust_token_indices(tensor, vocab_size):\n",
    "    adjusted_tensor = torch.clamp(tensor, min=0, max=vocab_size - 1)\n",
    "    return adjusted_tensor\n",
    "\n",
    "df['input_word_ids'] = df['input_word_ids'].apply(adjust_token_indices, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo preentrenado\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_toxicity_individual(df, model):\n",
    "    model.to('cpu')  # Asegura que el modelo se ejecute en la CPU\n",
    "    toxicity_scores = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # Extrae cada fila como un tensor individual\n",
    "        input_ids = df['input_word_ids'].iloc[i].unsqueeze(0)  # Añade una dimensión de lote\n",
    "        attention_mask = df['input_mask'].iloc[i].unsqueeze(0)  # Añade una dimensión de lote\n",
    "        token_type_ids = df['all_segment_id'].iloc[i].unsqueeze(0)  # Añade una dimensión de lote\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            score = probabilities[:, 1].item()  # Obtiene la probabilidad de la clase 1 como un número\n",
    "            toxicity_scores.append(score)\n",
    "    \n",
    "    return toxicity_scores\n",
    "\n",
    "# Aplicar la función predict_toxicity al DataFrame completo\n",
    "df_input = df.iloc[-10:]\n",
    "\n",
    "result = predict_toxicity_individual(df_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment_text  toxic  \\\n",
      "223539  Balancing the two approaches to psychiatry ( b...      0   \n",
      "223540  == Your name mentioned == \\n Hi, I just though...      0   \n",
      "223541  I've just discovered yet another list: List of...      0   \n",
      "223542  ::Consensus for ruining Wikipedia? I think tha...      0   \n",
      "223543  shut down the mexican border withought looking...      0   \n",
      "223544  :Jerome, I see you never got around to this…! ...      0   \n",
      "223545  ==Lucky bastard== \\n http://wikimediafoundatio...      0   \n",
      "223546  ==shame on you all!!!== \\n\\n You want to speak...      0   \n",
      "223547  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...      1   \n",
      "223548  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...      0   \n",
      "\n",
      "        toxicity_score  \n",
      "223539        0.504209  \n",
      "223540        0.491924  \n",
      "223541        0.513684  \n",
      "223542        0.494272  \n",
      "223543        0.493918  \n",
      "223544        0.517015  \n",
      "223545        0.510337  \n",
      "223546        0.506898  \n",
      "223547        0.534275  \n",
      "223548        0.514192  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21282/1083717648.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input['toxicity_score'] = result\n"
     ]
    }
   ],
   "source": [
    "df_input['toxicity_score'] = result\n",
    "print(df_input[['comment_text', 'toxic', 'toxicity_score']])\n",
    "\n",
    "# df['toxicity_score'] = result\n",
    "# print(df[['comment_text', 'toxicity_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['comment_text'].tolist()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"\", padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
